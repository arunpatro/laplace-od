{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision.datasets.folder import pil_loader\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "from helpers.utils import load_json\n",
    "\n",
    "class_map = {\n",
    "    \"bus\": 1,\n",
    "    \"car\": 2,\n",
    "    \"motor\": 3,\n",
    "    \"person\": 4,\n",
    "    \"rider\": 5,\n",
    "    \"traffic light\": 6,\n",
    "    \"traffic sign\": 7,\n",
    "    \"bike\": 8,\n",
    "    \"truck\": 9,\n",
    "}\n",
    "\n",
    "class BDDDataset(data.Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.samples = None\n",
    "        self.prepare()\n",
    "\n",
    "    def prepare(self):\n",
    "        self.samples = []\n",
    "\n",
    "        if self.train:\n",
    "            annotations = load_json(os.path.join(self.root, \"labels/bdd100k_labels_images_train.json\"))\n",
    "            annotations = annotations[:1000]\n",
    "            image_dir = os.path.join(self.root, \"images/100k/train\")\n",
    "        else:\n",
    "            annotations = load_json(os.path.join(self.root, \"labels/bdd100k_labels_images_val.json\"))\n",
    "            annotations = annotations[:10000]\n",
    "            image_dir = os.path.join(self.root, \"images/100k/val\")\n",
    "\n",
    "        for (idx, ann) in enumerate(annotations):\n",
    "            ## filter instances of \"lane\" and \"drivable_area\", because they have poly2d instead of box2d\n",
    "            invalid_idxs = [i for i, x in enumerate(ann[\"labels\"]) if x[\"category\"] in [\"lane\", \"drivable area\", \"train\"]]\n",
    "            if len(invalid_idxs) == len(ann[\"labels\"]):\n",
    "                continue\n",
    "            \n",
    "            ann[\"labels\"] = [ann[\"labels\"][i] for i in range(len(ann[\"labels\"])) if i not in invalid_idxs]\n",
    "            \n",
    "            target = {}\n",
    "            target[\"boxes\"] = [ann['labels'][i]['box2d'] for i in range(len(ann['labels']))]\n",
    "            target[\"boxes\"] = [[box[\"x1\"], box[\"y1\"], box[\"x2\"], box[\"y2\"]] for box in target[\"boxes\"]]\n",
    "            target[\"labels\"] = [class_map[ann['labels'][i]['category']] for i in range(len(ann['labels']))]\n",
    "            target[\"image_id\"] = idx + 1\n",
    "            target[\"area\"] = [(box[3] - box[1]) * (box[2] - box[0]) for box in target[\"boxes\"]]\n",
    "            target[\"iscrowd\"] = [0 for _ in target[\"boxes\"]]\n",
    "            # no mask        \n",
    "            \n",
    "            image_path = os.path.join(image_dir, ann[\"name\"])\n",
    "                        \n",
    "            if os.path.exists(image_path):\n",
    "                self.samples.append((image_path, target))\n",
    "            else:\n",
    "                raise FileNotFoundError\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, annotation = self.samples[index]\n",
    "\n",
    "        image = pil_loader(image_path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # image = tv_tensors.Image(image)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.tensor(annotation[\"boxes\"], dtype=torch.float).clone().detach()\n",
    "        target[\"labels\"] = torch.tensor(annotation[\"labels\"], dtype=torch.int64).clone().detach()\n",
    "        target[\"area\"] = torch.tensor(annotation[\"area\"], dtype=torch.float).clone().detach()\n",
    "        target[\"iscrowd\"] = torch.tensor(annotation[\"iscrowd\"], dtype=torch.int64).clone().detach()\n",
    "        target[\"image_id\"] = annotation[\"image_id\"]\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images, annotations = zip(*batch)\n",
    "    images = data.dataloader.default_collate(images)\n",
    "    annotations = list(annotations)\n",
    "    return images, annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# transform = transforms.Compose([transforms.Resize(64), transforms.ToTensor()])\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "loader_train = data.DataLoader(\n",
    "    BDDDataset(\"../data/bdd100k\", transform=transform), batch_size=32, shuffle=True, num_workers=10, collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "loader_val = data.DataLoader(\n",
    "    BDDDataset(\"../data/bdd100k\", transform=transform, train=False), batch_size=96, shuffle=False, num_workers=10, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, annotations = next(iter(loader_train))\n",
    "# model.train()\n",
    "# output = model(images, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## visualize the bboxes \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# def draw_boxes(image, annotation):\n",
    "#     \"\"\"\n",
    "#     Draws bounding boxes on the image.\n",
    "#     :param image: PIL image\n",
    "#     :param annotation: Annotation data for the image\n",
    "#     :return: Image with bounding boxes\n",
    "#     \"\"\"\n",
    "#     # Convert PIL Image to a matplotlib object\n",
    "#     fig, ax = plt.subplots(1)\n",
    "#     ax.imshow(image)\n",
    "\n",
    "#     for box in annotation['boxes']:\n",
    "#         rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n",
    "#         ax.add_patch(rect)\n",
    "\n",
    "#     return fig\n",
    "\n",
    "\n",
    "# for i, (images, annotations) in enumerate(loader_val):\n",
    "#     if i == 0:  # Visualize the first batch\n",
    "#         for j in range(len(images)):\n",
    "#             image = transforms.functional.to_pil_image(images[j])\n",
    "#             annotation = annotations[j]\n",
    "#             fig = draw_boxes(image, annotation)\n",
    "#             plt.show()\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = load_json(os.path.join(\"../bdd100k\", \"labels/bdd100k_labels_images_val.json\"))\n",
    "# classes = []\n",
    "# for val in vals:\n",
    "#     for label in val['labels']:\n",
    "#         classes.append(label['category'])\n",
    "        \n",
    "# from collections import Counter\n",
    "# counts = Counter(classes)\n",
    "# # hbar\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.barh(list(counts.keys()), list(counts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train params / params  = 18912333 / 18971229\n"
     ]
    }
   ],
   "source": [
    "## do a baseline model\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1)\n",
    "\n",
    "num_classes = 9 + 1\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "\n",
    "## num of params\n",
    "print(f\"train params / params  = {sum(p.numel() for p in model.parameters() if p.requires_grad)} / {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): Conv2dNormActivation(\n",
       "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-1): 2 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=10, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# for (images, targets) in loader_val:\n",
    "#     images = list(image.to(device) for image in images)\n",
    "#     targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "#     outputs = model(images, targets)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# for (images, annotations) in loader_train:\n",
    "#     images = list(image.to(device) for image in images)\n",
    "#     annotations = [{k: v.to(device) for k, v in t.items() if isinstance(v, torch.Tensor)} for t in annotations]\n",
    "#     outputs = model(images)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "from netcal.metrics import ECE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:19<00:00, 125.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from helpers.coco_eval import CocoEvaluator\n",
    "from helpers.coco_utils import get_coco_api_from_dataset\n",
    "import pickle\n",
    "\n",
    "# with open('bdd_coco_evaluator.pkl', 'rb') as f:\n",
    "#     coco_evaluator = pickle.load(f)['coco']\n",
    "    \n",
    "iou_types = [\"bbox\"]\n",
    "coco = get_coco_api_from_dataset(loader_val.dataset)\n",
    "coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "# with open('bdd_coco_evaluator.pkl', 'wb') as f:\n",
    "#     pickle.dump({\"coco\": coco}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# images, targets = next(iter(loader_train))\n",
    "# images = list(image.to(device) for image in images)\n",
    "# targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "# loss_dict = model(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnura-ortap\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/nyu/laplace-od/src/wandb/run-20231218_102913-atebeupk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nura-ortap/bml-od/runs/atebeupk' target=\"_blank\">baseline-2023-12-18_10-29-12</a></strong> to <a href='https://wandb.ai/nura-ortap/bml-od' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nura-ortap/bml-od' target=\"_blank\">https://wandb.ai/nura-ortap/bml-od</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nura-ortap/bml-od/runs/atebeupk' target=\"_blank\">https://wandb.ai/nura-ortap/bml-od/runs/atebeupk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "expt_name = \"baseline\"\n",
    "time_stamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run = wandb.init(project='bml-od', name=f\"{expt_name}-{time_stamp}\", config={\n",
    "    \"model\": \"fasterrcnn_mobilenet_v3_large_fpn\",\n",
    "    \"dataset\": \"bdd100k\",\n",
    "    \"data_subset\": \"5k\",\n",
    "    \"model_subset\": \"all\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "full_loss_dict = []\n",
    "full_ece_data = []\n",
    "for images, targets in loader_train:\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [\n",
    "        {\n",
    "            k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "            for k, v in t.items()\n",
    "        }\n",
    "        for t in targets\n",
    "    ]\n",
    "    with torch.cuda.amp.autocast(False), torch.inference_mode():\n",
    "        loss_dict, ece_data = model(images, targets)\n",
    "        full_loss_dict.append(loss_dict)\n",
    "        full_ece_data.append(ece_data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss_dict = {\n",
    "    k: torch.stack([loss_dict[k] for loss_dict in full_loss_dict]).mean()\n",
    "    for k in full_loss_dict[0].keys()\n",
    "}\n",
    "\n",
    "final_ece_data = {}\n",
    "for layer in ['prop', 'det']:\n",
    "    final_ece_data[layer] = {\n",
    "        'probs': torch.hstack([ece_data[layer]['probs'] for ece_data in full_ece_data]),\n",
    "        'labels': torch.hstack([ece_data[layer]['labels'] for ece_data in full_ece_data]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netcal.metrics import ECE\n",
    "from helpers.engine import get_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_ece = ECE(bins=10).measure(final_ece_data['prop']['probs'].detach().cpu().numpy(), final_ece_data['prop']['labels'].detach().cpu().numpy())\n",
    "det_ece = ECE(bins=10).measure(final_ece_data['det']['probs'].detach().cpu().numpy(), final_ece_data['det']['labels'].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def get_metrics(model, data_loader, device):\n",
    "    model.train()\n",
    "\n",
    "    full_loss_dict = []\n",
    "    full_ece_data = []\n",
    "    for images, targets in tqdm(data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [\n",
    "            {\n",
    "                k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                for k, v in t.items()\n",
    "            }\n",
    "            for t in targets\n",
    "        ]\n",
    "        with torch.cuda.amp.autocast(False), torch.inference_mode():\n",
    "            loss_dict, ece_data = model(images, targets)\n",
    "            full_loss_dict.append(loss_dict)\n",
    "            full_ece_data.append(ece_data)\n",
    "\n",
    "    final_ece_data = {}\n",
    "    for layer in ['prop', 'det']:\n",
    "        final_ece_data[layer] = {\n",
    "            'probs': torch.hstack([ece_data[layer]['probs'] for ece_data in full_ece_data]),\n",
    "            'labels': torch.hstack([ece_data[layer]['labels'] for ece_data in full_ece_data]),\n",
    "        }\n",
    "        \n",
    "    metrics = {\n",
    "        \"losses\": {\n",
    "        k: torch.stack([loss_dict[k] for loss_dict in full_loss_dict]).mean()\n",
    "        for k in full_loss_dict[0].keys()\n",
    "    },\n",
    "        \"ece_data\": final_ece_data\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:52<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "t_metrics = get_metrics(model, loader_train, device)\n",
    "v_metrics = get_metrics(model, loader_val, device)\n",
    "\n",
    "train_prop_ece = ECE(bins=10).measure(t_metrics['ece_data']['prop']['probs'].detach().cpu().numpy(), t_metrics['ece_data']['prop']['labels'].detach().cpu().numpy())\n",
    "train_det_ece = ECE(bins=10).measure(t_metrics['ece_data']['det']['probs'].detach().cpu().numpy(), t_metrics['ece_data']['det']['labels'].detach().cpu().numpy())\n",
    "\n",
    "val_prop_ece = ECE(bins=10).measure(v_metrics['ece_data']['prop']['probs'].detach().cpu().numpy(), v_metrics['ece_data']['prop']['labels'].detach().cpu().numpy())\n",
    "val_det_ece = ECE(bins=10).measure(v_metrics['ece_data']['det']['probs'].detach().cpu().numpy(), v_metrics['ece_data']['det']['labels'].detach().cpu().numpy())\n",
    "\n",
    "run.log({\n",
    "    \"train_losses\": t_metrics[\"losses\"],\n",
    "    \"val_losses\": v_metrics[\"losses\"],\n",
    "    \"train_prop_ece\": train_prop_ece,\n",
    "    \"train_det_ece\": train_det_ece,\n",
    "    \"val_prop_ece\": val_prop_ece,\n",
    "    \"val_det_ece\": val_det_ece,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(2.5136, device='cuda:0'),\n",
       " 'loss_box_reg': tensor(0.7167, device='cuda:0'),\n",
       " 'loss_objectness': tensor(0.1477, device='cuda:0'),\n",
       " 'loss_rpn_box_reg': tensor(0.1049, device='cuda:0')}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_metrics[\"losses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log({\n",
    "    \"train_losses\": t_metrics[\"losses\"],\n",
    "    \"val_losses\": v_metrics[\"losses\"],\n",
    "    \"train_prop_ece\": train_prop_ece,\n",
    "    \"train_det_ece\": train_det_ece,\n",
    "    \"val_prop_ece\": val_prop_ece,\n",
    "    \"val_det_ece\": val_det_ece,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/nyu/laplace-od/src/wandb/run-20231218_105917-1yy7bc6j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nura-ortap/bml-od/runs/1yy7bc6j' target=\"_blank\">baseline-2023-12-18_10-29-122</a></strong> to <a href='https://wandb.ai/nura-ortap/bml-od' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nura-ortap/bml-od' target=\"_blank\">https://wandb.ai/nura-ortap/bml-od</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nura-ortap/bml-od/runs/1yy7bc6j' target=\"_blank\">https://wandb.ai/nura-ortap/bml-od/runs/1yy7bc6j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='bml-od', name=f\"{expt_name}-{time_stamp}2\", config={\n",
    "    \"model\": \"fasterrcnn_mobilenet_v3_large_fpn\",\n",
    "    \"dataset\": \"bdd100k\",\n",
    "    \"data_subset\": \"5k\",\n",
    "    \"model_subset\": \"all\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diagla.curvatures import Diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = Diagonal(model, layer_types=['Linear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:15<00:00,  2.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from helpers import utils\n",
    "model.train()\n",
    "lr_scheduler = None\n",
    "for images, targets in tqdm(loader_train):\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [\n",
    "        {\n",
    "            k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "            for k, v in t.items()\n",
    "        }\n",
    "        for t in targets\n",
    "    ]\n",
    "    with torch.cuda.amp.autocast(False):\n",
    "        loss_dict, ece_data = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "    # reduce losses over all GPUs for logging purposes\n",
    "    loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "    losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "    loss_value = losses_reduced.item()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    diag.update(batch_size=len(images))\n",
    "\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag.invert(add=0.1, multiply=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdiag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nyu/laplace-od/src/diagla/curvatures.py:198\u001b[0m, in \u001b[0;36mDiagonal.sample\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    196\u001b[0m            layer: Union[Module, \u001b[38;5;28mstr\u001b[39m]):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minv_state, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInverse state dict is empty. Did you call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvert\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m prior to this?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minv_state[layer]\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m.\u001b[39mnormal_() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minv_state[layer]\n",
      "\u001b[0;31mKeyError\u001b[0m: 1000"
     ]
    }
   ],
   "source": [
    "diag.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<laplace.lllaplace.KronLLLaplace at 0x7f066866abc0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "la.optimize_prior_precision(method=\"marglik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss_classifier': tensor(2.5136, device='cuda:0'),\n",
       "  'loss_box_reg': tensor(0.7163, device='cuda:0'),\n",
       "  'loss_objectness': tensor(0.1479, device='cuda:0'),\n",
       "  'loss_rpn_box_reg': tensor(0.1048, device='cuda:0')},\n",
       " {'prop': {'probs': tensor([0.5536, 0.6188, 0.5925,  ..., 0.0371, 0.9870, 0.2780], device='cuda:0'),\n",
       "   'labels': tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0')},\n",
       "  'det': {'probs': tensor([0.1404, 0.1438, 0.1657,  ..., 0.1734, 0.1506, 0.1387], device='cuda:0'),\n",
       "   'labels': tensor([2, 2, 2,  ..., 2, 7, 7], device='cuda:0')}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_loss_dict, final_ece_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train it just for 2 epochs\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_metrics = train_one_epoch(model, optimizer, loader_train, device, epoch, print_freq=10, wandbrun=run)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    eval_metrics = evaluate(model, loader_val, device, coco_evaluator)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = get_metrics(model, loader_train, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_loss_dict, full_ece_data = get_metrics(model, loader_val, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ece_data[1]['det']['labels'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[torch.vstack(full_ece_data[i]['prop']['probs']) for i in range(len(full_ece_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['prop', 'det']:\n",
    "    probs = torch.hstack([full_ece_data[i][k]['probs'][0] for i in range(len(full_ece_data))])\n",
    "    labels = torch.hstack([full_ece_data[i][k]['labels'][0] for i in range(len(full_ece_data))])\n",
    "    # print(probs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    k: {\n",
    "         \"probs\": torch.vstack([full_ece_data[0][k][i][\"probs\"] for i in range(len(full_ece_data[0][k]))]),\n",
    "         \"labels\": torch.hstack([full_ece_data[0][k][i][\"labels\"] for i in range(len(full_ece_data[0][k]))]),\n",
    "    }\n",
    "    for k in full_ece_data[0].keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def get_metrics(model, data_loader, device):\n",
    "    model.train()\n",
    "\n",
    "    full_loss_dict = []\n",
    "    full_ece_data = []\n",
    "    for images, targets in tqdm(data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [\n",
    "            {\n",
    "                k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                for k, v in t.items()\n",
    "            }\n",
    "            for t in targets\n",
    "        ]\n",
    "        with torch.cuda.amp.autocast(False), torch.inference_mode():\n",
    "            loss_dict, ece_data = model(images, targets)\n",
    "            full_loss_dict.append(loss_dict)\n",
    "            full_ece_data.append(ece_data)\n",
    "\n",
    "    # metrics = {\n",
    "    #     \"losses\": {\n",
    "    #         k: torch.stack([d[k] for d in full_loss_dict]).mean().item()\n",
    "    #         for k in full_loss_dict[0].keys()\n",
    "    #     },\n",
    "    #     \"ece_data\": {\n",
    "    #         k: torch.vstack([d[k] for d in full_ece_data])\n",
    "    #         for k in full_ece_data[0].keys()\n",
    "    #     }\n",
    "    # }\n",
    "    \n",
    "    \n",
    "\n",
    "    return full_loss_dict, full_ece_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(loader_val))\n",
    "\n",
    "model.train()\n",
    "outputs = model(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics.meters['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval0.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(loader_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = load_json(os.path.join(\"../data/bdd100k/\", \"labels/bdd100k_labels_images_val.json\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "for ann in annotations:\n",
    "    for label in ann['labels']:\n",
    "        categories.append(label['category'])\n",
    "    \n",
    "from collections import Counter\n",
    "print(Counter(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
