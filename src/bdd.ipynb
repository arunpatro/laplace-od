{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision.datasets.folder import pil_loader\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "from utils import load_json\n",
    "\n",
    "class_map = {\n",
    "    \"bike\": 0,\n",
    "    \"bus\": 1,\n",
    "    \"car\": 2,\n",
    "    \"motor\": 3,\n",
    "    \"person\": 4,\n",
    "    \"rider\": 5,\n",
    "    \"traffic light\": 6,\n",
    "    \"traffic sign\": 7,\n",
    "    \"train\": 8,\n",
    "    \"truck\": 9,\n",
    "}\n",
    "\n",
    "class BDDDataset(data.Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.samples = None\n",
    "        self.prepare()\n",
    "\n",
    "    def prepare(self):\n",
    "        self.samples = []\n",
    "\n",
    "        if self.train:\n",
    "            annotations = load_json(os.path.join(self.root, \"labels/bdd100k_labels_images_train.json\"))\n",
    "            annotations = annotations[:5000]\n",
    "            image_dir = os.path.join(self.root, \"images/100k/train\")\n",
    "        else:\n",
    "            annotations = load_json(os.path.join(self.root, \"labels/bdd100k_labels_images_val.json\"))\n",
    "            annotations = annotations[:5000]\n",
    "            image_dir = os.path.join(self.root, \"images/100k/val\")\n",
    "\n",
    "        for (idx, ann) in enumerate(annotations):\n",
    "            ## filter instances of \"lane\" and \"drivable_area\", because they have poly2d instead of box2d\n",
    "            invalid_idxs = [i for i, x in enumerate(ann[\"labels\"]) if x[\"category\"] in [\"lane\", \"drivable area\"]]\n",
    "            if len(invalid_idxs) == len(ann[\"labels\"]):\n",
    "                continue\n",
    "            \n",
    "            ann[\"labels\"] = [ann[\"labels\"][i] for i in range(len(ann[\"labels\"])) if i not in invalid_idxs]\n",
    "            \n",
    "            \n",
    "            target = {}\n",
    "            target[\"boxes\"] = [ann['labels'][i]['box2d'] for i in range(len(ann['labels']))]\n",
    "            target[\"boxes\"] = [[box[\"x1\"], box[\"y1\"], box[\"x2\"], box[\"y2\"]] for box in target[\"boxes\"]]\n",
    "            target[\"labels\"] = [class_map[ann['labels'][i]['category']] for i in range(len(ann['labels']))]\n",
    "            target[\"image_id\"] = idx + 1\n",
    "            target[\"area\"] = [(box[3] - box[1]) * (box[2] - box[0]) for box in target[\"boxes\"]]\n",
    "            target[\"iscrowd\"] = [0 for _ in target[\"boxes\"]]\n",
    "            # no mask        \n",
    "            \n",
    "            image_path = os.path.join(image_dir, ann[\"name\"])\n",
    "                        \n",
    "            if os.path.exists(image_path):\n",
    "                self.samples.append((image_path, target))\n",
    "            else:\n",
    "                raise FileNotFoundError\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # TODO: handle label dict\n",
    "\n",
    "        image_path, annotation = self.samples[index]\n",
    "\n",
    "        image = pil_loader(image_path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        image = tv_tensors.Image(image)\n",
    "        annotation[\"boxes\"] = torch.tensor(annotation[\"boxes\"], dtype=torch.float)\n",
    "        annotation[\"labels\"] = torch.tensor(annotation[\"labels\"], dtype=torch.int64)\n",
    "        annotation[\"area\"] = torch.tensor(annotation[\"area\"], dtype=torch.float)\n",
    "        annotation[\"iscrowd\"] = torch.tensor(annotation[\"iscrowd\"], dtype=torch.int64)\n",
    "        annotation[\"image_id\"] = annotation[\"image_id\"]\n",
    "\n",
    "        return image, annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images, annotations = zip(*batch)\n",
    "    images = data.dataloader.default_collate(images)\n",
    "    annotations = list(annotations)\n",
    "    return images, annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# transform = transforms.Compose([transforms.Resize(64), transforms.ToTensor()])\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "loader_train = data.DataLoader(\n",
    "    BDDDataset(\"../bdd100k\", transform=transform), batch_size=64, shuffle=True, num_workers=8, collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "loader_val = data.DataLoader(\n",
    "    BDDDataset(\"../bdd100k\", transform=transform, train=False), batch_size=64, shuffle=False, num_workers=8, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## visualize the bboxes \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# def draw_boxes(image, annotation):\n",
    "#     \"\"\"\n",
    "#     Draws bounding boxes on the image.\n",
    "#     :param image: PIL image\n",
    "#     :param annotation: Annotation data for the image\n",
    "#     :return: Image with bounding boxes\n",
    "#     \"\"\"\n",
    "#     # Convert PIL Image to a matplotlib object\n",
    "#     fig, ax = plt.subplots(1)\n",
    "#     ax.imshow(image)\n",
    "\n",
    "#     for box in annotation['boxes']:\n",
    "#         rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n",
    "#         ax.add_patch(rect)\n",
    "\n",
    "#     return fig\n",
    "\n",
    "\n",
    "# for i, (images, annotations) in enumerate(loader_val):\n",
    "#     if i == 0:  # Visualize the first batch\n",
    "#         for j in range(len(images)):\n",
    "#             image = transforms.functional.to_pil_image(images[j])\n",
    "#             annotation = annotations[j]\n",
    "#             fig = draw_boxes(image, annotation)\n",
    "#             plt.show()\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = load_json(os.path.join(\"../bdd100k\", \"labels/bdd100k_labels_images_val.json\"))\n",
    "# classes = []\n",
    "# for val in vals:\n",
    "#     for label in val['labels']:\n",
    "#         classes.append(label['category'])\n",
    "        \n",
    "# from collections import Counter\n",
    "# counts = Counter(classes)\n",
    "# # hbar\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.barh(list(counts.keys()), list(counts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/bml/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/paperspace/miniconda3/envs/bml/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18917458"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## do a baseline model\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "\n",
    "num_classes = 10 + 1\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model = model.cuda()\n",
    "## num of params\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model.eval()\n",
    "# model.train()\n",
    "# for (images, annotations) in loader_train:\n",
    "#     images = list(image.to(device) for image in images)\n",
    "#     # print([i.shape for i in images])\n",
    "#     annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "#     outputs = model(images, annotations)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = next(iter(loader_val))\n",
    "# b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 4858/10000 [00:34<00:37, 137.18it/s]"
     ]
    }
   ],
   "source": [
    "from coco_eval import CocoEvaluator\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "\n",
    "iou_types = [\"bbox\"]\n",
    "coco = get_coco_api_from_dataset(loader_val.dataset)\n",
    "coco_evaluator = CocoEvaluator(coco, iou_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train it just for 2 epochs\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, loader_train, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, loader_val, device, coco_evaluator)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
