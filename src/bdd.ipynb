{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision.datasets.folder import pil_loader\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "from helpers.utils import load_json\n",
    "\n",
    "class_map = {\n",
    "    \"bus\": 1,\n",
    "    \"car\": 2,\n",
    "    \"motor\": 3,\n",
    "    \"person\": 4,\n",
    "    \"rider\": 5,\n",
    "    \"traffic light\": 6,\n",
    "    \"traffic sign\": 7,\n",
    "    \"bike\": 8,\n",
    "    \"truck\": 9,\n",
    "}\n",
    "\n",
    "class BDDDataset(data.Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.samples = None\n",
    "        self.prepare()\n",
    "\n",
    "    def prepare(self):\n",
    "        self.samples = []\n",
    "\n",
    "        if self.train:\n",
    "            annotations = load_json(os.path.join(self.root, \"labels/bdd100k_labels_images_train.json\"))\n",
    "            annotations = annotations[:5000]\n",
    "            image_dir = os.path.join(self.root, \"images/100k/train\")\n",
    "        else:\n",
    "            annotations = load_json(os.path.join(self.root, \"labels/bdd100k_labels_images_val.json\"))\n",
    "            # annotations = annotations[:5000]\n",
    "            image_dir = os.path.join(self.root, \"images/100k/val\")\n",
    "\n",
    "        for (idx, ann) in enumerate(annotations):\n",
    "            ## filter instances of \"lane\" and \"drivable_area\", because they have poly2d instead of box2d\n",
    "            invalid_idxs = [i for i, x in enumerate(ann[\"labels\"]) if x[\"category\"] in [\"lane\", \"drivable area\", \"train\"]]\n",
    "            if len(invalid_idxs) == len(ann[\"labels\"]):\n",
    "                continue\n",
    "            \n",
    "            ann[\"labels\"] = [ann[\"labels\"][i] for i in range(len(ann[\"labels\"])) if i not in invalid_idxs]\n",
    "            \n",
    "            target = {}\n",
    "            target[\"boxes\"] = [ann['labels'][i]['box2d'] for i in range(len(ann['labels']))]\n",
    "            target[\"boxes\"] = [[box[\"x1\"], box[\"y1\"], box[\"x2\"], box[\"y2\"]] for box in target[\"boxes\"]]\n",
    "            target[\"labels\"] = [class_map[ann['labels'][i]['category']] for i in range(len(ann['labels']))]\n",
    "            target[\"image_id\"] = idx + 1\n",
    "            target[\"area\"] = [(box[3] - box[1]) * (box[2] - box[0]) for box in target[\"boxes\"]]\n",
    "            target[\"iscrowd\"] = [0 for _ in target[\"boxes\"]]\n",
    "            # no mask        \n",
    "            \n",
    "            image_path = os.path.join(image_dir, ann[\"name\"])\n",
    "                        \n",
    "            if os.path.exists(image_path):\n",
    "                self.samples.append((image_path, target))\n",
    "            else:\n",
    "                raise FileNotFoundError\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, annotation = self.samples[index]\n",
    "\n",
    "        image = pil_loader(image_path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # image = tv_tensors.Image(image)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.tensor(annotation[\"boxes\"], dtype=torch.float).clone().detach()\n",
    "        target[\"labels\"] = torch.tensor(annotation[\"labels\"], dtype=torch.int64).clone().detach()\n",
    "        target[\"area\"] = torch.tensor(annotation[\"area\"], dtype=torch.float).clone().detach()\n",
    "        target[\"iscrowd\"] = torch.tensor(annotation[\"iscrowd\"], dtype=torch.int64).clone().detach()\n",
    "        target[\"image_id\"] = annotation[\"image_id\"]\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images, annotations = zip(*batch)\n",
    "    images = data.dataloader.default_collate(images)\n",
    "    annotations = list(annotations)\n",
    "    return images, annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# transform = transforms.Compose([transforms.Resize(64), transforms.ToTensor()])\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "loader_train = data.DataLoader(\n",
    "    BDDDataset(\"../data/bdd100k\", transform=transform), batch_size=32, shuffle=True, num_workers=10, collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "loader_val = data.DataLoader(\n",
    "    BDDDataset(\"../data/bdd100k\", transform=transform, train=False), batch_size=32, shuffle=False, num_workers=10, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, annotations = next(iter(loader_train))\n",
    "# model.train()\n",
    "# output = model(images, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## visualize the bboxes \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# def draw_boxes(image, annotation):\n",
    "#     \"\"\"\n",
    "#     Draws bounding boxes on the image.\n",
    "#     :param image: PIL image\n",
    "#     :param annotation: Annotation data for the image\n",
    "#     :return: Image with bounding boxes\n",
    "#     \"\"\"\n",
    "#     # Convert PIL Image to a matplotlib object\n",
    "#     fig, ax = plt.subplots(1)\n",
    "#     ax.imshow(image)\n",
    "\n",
    "#     for box in annotation['boxes']:\n",
    "#         rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n",
    "#         ax.add_patch(rect)\n",
    "\n",
    "#     return fig\n",
    "\n",
    "\n",
    "# for i, (images, annotations) in enumerate(loader_val):\n",
    "#     if i == 0:  # Visualize the first batch\n",
    "#         for j in range(len(images)):\n",
    "#             image = transforms.functional.to_pil_image(images[j])\n",
    "#             annotation = annotations[j]\n",
    "#             fig = draw_boxes(image, annotation)\n",
    "#             plt.show()\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = load_json(os.path.join(\"../bdd100k\", \"labels/bdd100k_labels_images_val.json\"))\n",
    "# classes = []\n",
    "# for val in vals:\n",
    "#     for label in val['labels']:\n",
    "#         classes.append(label['category'])\n",
    "        \n",
    "# from collections import Counter\n",
    "# counts = Counter(classes)\n",
    "# # hbar\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.barh(list(counts.keys()), list(counts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "print(torchvision.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do a baseline model\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1)\n",
    "\n",
    "num_classes = 9 + 1\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "\n",
    "## num of params\n",
    "print(f\"train params / params  = {sum(p.numel() for p in model.parameters() if p.requires_grad)} / {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.train()\n",
    "for (images, targets) in loader_train:\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "    outputs = model(images, targets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for (images, annotations) in loader_train:\n",
    "    images = list(image.to(device) for image in images)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items() if isinstance(v, torch.Tensor)} for t in annotations]\n",
    "    outputs = model(images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "from netcal.metrics import ECE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = box_iou(outputs[0]['boxes'], annotations[0]['boxes'])#.shape\n",
    "# true_positives = ious.max(dim=-1)\n",
    "tp_inds = torch.where(ious.max(dim=-1)[0] > 0.5)[0]\n",
    "matched_inds = ious.max(dim=-1)[1][tp_inds]\n",
    "probs = outputs[0]['scores'][tp_inds]\n",
    "\n",
    "\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_gt_labels = annotations[0]['labels'][matched_inds]\n",
    "matched_gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]['boxes'].shape, annotations[0]['boxes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = next(iter(loader_val))\n",
    "# b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.coco_eval import CocoEvaluator\n",
    "from helpers.coco_utils import get_coco_api_from_dataset\n",
    "import pickle\n",
    "\n",
    "# with open('bdd_coco_evaluator.pkl', 'rb') as f:\n",
    "#     coco_evaluator = pickle.load(f)['coco']\n",
    "    \n",
    "iou_types = [\"bbox\"]\n",
    "coco = get_coco_api_from_dataset(loader_val.dataset)\n",
    "coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "# with open('bdd_coco_evaluator.pkl', 'wb') as f:\n",
    "#     pickle.dump({\"coco\": coco}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# images, targets = next(iter(loader_train))\n",
    "# images = list(image.to(device) for image in images)\n",
    "# targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "# loss_dict = model(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "expt_name = \"baseline\"\n",
    "time_stamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run = wandb.init(project='bml-od', name=f\"{expt_name}-{time_stamp}\", config={\n",
    "    \"model\": \"fasterrcnn_mobilenet_v3_large_fpn\",\n",
    "    \"dataset\": \"bdd100k\",\n",
    "    \"model_subset\": \"all\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train it just for 2 epochs\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_metrics = train_one_epoch(model, optimizer, loader_train, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    eval_metrics = evaluate(model, loader_val, device, coco_evaluator)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, loader_val, device, coco_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(loader_val))\n",
    "\n",
    "model.train()\n",
    "outputs = model(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics.meters['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval0.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(loader_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = load_json(os.path.join(\"../data/bdd100k/\", \"labels/bdd100k_labels_images_val.json\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "for ann in annotations:\n",
    "    for label in ann['labels']:\n",
    "        categories.append(label['category'])\n",
    "    \n",
    "from collections import Counter\n",
    "print(Counter(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
